{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaimurRahmanR/StartUpSuccessPredictor/blob/main/Naimur_Dissertation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeUDDmmowkkp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load each dataset\n",
        "datasets = {\n",
        "    \"acquisitions\": \"/content/acquisitions.csv\",\n",
        "    \"degrees\": \"/content/degrees.csv\",\n",
        "    \"funding_rounds\": \"/content/funding_rounds.csv\",\n",
        "    \"funds\": \"/content/funds.csv\",\n",
        "    \"investments\": \"/content/investments.csv\",\n",
        "    \"ipos\": \"/content/ipos.csv\",\n",
        "    \"milestones\": \"/content/milestones.csv\",\n",
        "    \"objects\": \"/content/objects.csv\",\n",
        "    \"offices\": \"/content/offices.csv\",\n",
        "    \"people\": \"/content/people.csv\"\n",
        "}\n",
        "\n",
        "# Reading and storing the dataframes\n",
        "dataframes = {name: pd.read_csv(path) for name, path in datasets.items()}\n",
        "\n",
        "# Displaying basic information about each dataframe\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"--- {name.upper()} DATAFRAME ---\")\n",
        "    df.info()\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRQvYVbE0MUu"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Assuming 'dataframes' is your dictionary of datasets\n",
        "\n",
        "# Create a dictionary to store the occurrence of each column in different dataframes\n",
        "column_occurrences = defaultdict(set)\n",
        "\n",
        "# Iterate over each dataset and its columns\n",
        "for name, df in dataframes.items():\n",
        "    for column in df.columns:\n",
        "        column_occurrences[column].add(name)\n",
        "\n",
        "# Find and display common columns\n",
        "common_columns = {column: datasets for column, datasets in column_occurrences.items() if len(datasets) > 1}\n",
        "common_columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMc-52lJ1M-p"
      },
      "outputs": [],
      "source": [
        "# Potential column names that could represent the startup name\n",
        "name_columns = ['name', 'startup_name', 'company_name']\n",
        "\n",
        "# Check which datasets contain these columns\n",
        "datasets_with_name_column = {}\n",
        "for name, df in dataframes.items():\n",
        "    for col in name_columns:\n",
        "        if col in df.columns:\n",
        "            datasets_with_name_column[name] = col\n",
        "            break  # Found a suitable column, no need to check further\n",
        "\n",
        "# Display which datasets can be merged based on the startup name\n",
        "print(\"Datasets that can be merged based on the startup name:\")\n",
        "for dataset_name, column_name in datasets_with_name_column.items():\n",
        "    print(f\"- {dataset_name} (column: {column_name})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj4Q0iegLsOh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'dataframes' contains your datasets\n",
        "\n",
        "# Standardize the 'name' column (e.g., convert to lowercase for consistency)\n",
        "dataframes['objects']['name'] = dataframes['objects']['name'].str.lower()\n",
        "dataframes['funds']['name'] = dataframes['funds']['name'].str.lower()\n",
        "\n",
        "# Merge the datasets on the 'name' column\n",
        "merged_df = dataframes['objects'].merge(dataframes['funds'], on='name', how='left', suffixes=('', '_funds'))\n",
        "\n",
        "# Check the first few rows of the merged dataframe\n",
        "print(merged_df.head())\n",
        "\n",
        "# Save the unified dataset to a CSV file\n",
        "merged_df.to_csv('/content/funds_object.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_bpIvo3MXwa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the merged 'funds_object.csv' dataset\n",
        "funds_object_df = pd.read_csv('/content/funds_object.csv')\n",
        "\n",
        "# Assuming 'dataframes' is your dictionary of the other datasets\n",
        "# Let's add funds_object_df to this dictionary for ease of comparison\n",
        "dataframes['funds_object'] = funds_object_df\n",
        "\n",
        "# Function to find common columns between two dataframes\n",
        "def find_common_columns(df1, df2):\n",
        "    return set(df1.columns).intersection(df2.columns)\n",
        "\n",
        "# Compare the columns of 'funds_object' with each of the other datasets\n",
        "common_columns = {}\n",
        "for name, df in dataframes.items():\n",
        "    if name != 'funds_object':\n",
        "        common_cols = find_common_columns(funds_object_df, df)\n",
        "        if common_cols:\n",
        "            common_columns[name] = common_cols\n",
        "\n",
        "# Display the common columns found\n",
        "print(\"Common columns with 'funds_object' dataset:\")\n",
        "for dataset_name, columns in common_columns.items():\n",
        "    print(f\"- {dataset_name}: {', '.join(columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCSG1hsyNFQI"
      },
      "outputs": [],
      "source": [
        "# Assuming 'dataframes' is your dictionary of datasets\n",
        "\n",
        "# Check for 'object_id' in each dataset\n",
        "object_id_presence = {}\n",
        "for name, df in dataframes.items():\n",
        "    object_id_presence[name] = 'object_id' in df.columns\n",
        "\n",
        "# Display the presence of 'object_id' in each dataset\n",
        "for dataset, has_object_id in object_id_presence.items():\n",
        "    print(f\"{dataset}: {'Yes' if has_object_id else 'No'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxPDb-bbNnKn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'dataframes' is your dictionary of datasets\n",
        "# Assuming 'funds_object' dataset is already in 'dataframes' dictionary\n",
        "\n",
        "# Primary dataset (choose one with the most comprehensive 'object_id' coverage, e.g., 'funds_object')\n",
        "primary_df = dataframes['funds_object']\n",
        "\n",
        "# List of datasets to merge (which contain 'object_id')\n",
        "datasets_to_merge = ['degrees', 'funding_rounds', 'funds', 'ipos', 'milestones', 'offices', 'people']\n",
        "\n",
        "# Merge datasets with 'object_id'\n",
        "for dataset_name in datasets_to_merge:\n",
        "    if 'object_id' in dataframes[dataset_name].columns:\n",
        "        primary_df = primary_df.merge(dataframes[dataset_name], on='object_id', how='left', suffixes=('', f'_{dataset_name}'))\n",
        "\n",
        "# Check the first few rows of the merged dataframe\n",
        "print(primary_df.head())\n",
        "\n",
        "# Save the unified dataset to a CSV file\n",
        "primary_df.to_csv('/content/eight_in_one.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-sMTVQfPPle"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Load the merged 'eight_in_one' dataset\n",
        "eight_in_one_df = pd.read_csv('/content/eight_in_one.csv')\n",
        "\n",
        "# Update the dataframes dictionary\n",
        "dataframes['eight_in_one'] = eight_in_one_df\n",
        "\n",
        "# Function to find common columns between two dataframes\n",
        "def find_common_columns(df1, df2):\n",
        "    return set(df1.columns).intersection(df2.columns)\n",
        "\n",
        "# Identify common columns between 'eight_in_one' and 'acquisitions', 'investments'\n",
        "common_columns_acquisitions = find_common_columns(eight_in_one_df, dataframes['acquisitions'])\n",
        "common_columns_investments = find_common_columns(eight_in_one_df, dataframes['investments'])\n",
        "\n",
        "# Display the common columns\n",
        "print(\"Common columns with 'acquisitions':\", common_columns_acquisitions)\n",
        "print(\"Common columns with 'investments':\", common_columns_investments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66_rxKSaRUVn"
      },
      "outputs": [],
      "source": [
        "# Assuming 'dataframes' contains your dictionary of datasets\n",
        "# Assuming 'primary_df' is the final merged dataset\n",
        "\n",
        "# List of datasets to merge (which contain 'object_id')\n",
        "datasets_to_merge = ['degrees', 'funding_rounds', 'funds', 'ipos', 'milestones', 'offices', 'people']\n",
        "\n",
        "# Check which datasets have not been merged\n",
        "datasets_not_merged = [dataset for dataset in dataframes.keys() if dataset not in datasets_to_merge]\n",
        "\n",
        "# If 'funds_object' is in the list, remove it because it's the primary dataset\n",
        "if 'funds_object' in datasets_not_merged:\n",
        "    datasets_not_merged.remove('funds_object')\n",
        "\n",
        "# Print the datasets that have not been merged\n",
        "print(\"Datasets that have not been merged:\")\n",
        "for dataset in datasets_not_merged:\n",
        "    print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZiRPPx2RnSg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the merged 'eight_in_one.csv' dataset\n",
        "eight_in_one_df = pd.read_csv('/content/eight_in_one.csv')\n",
        "\n",
        "# Extract the column names containing suffixes\n",
        "suffixes = [col.split('_')[1] for col in eight_in_one_df.columns if '_' in col]\n",
        "\n",
        "# Deduplicate the suffixes list\n",
        "unique_suffixes = list(set(suffixes))\n",
        "\n",
        "# Identify the datasets merged based on their suffixes\n",
        "merged_datasets = [dataset for dataset in datasets_to_merge if dataset in unique_suffixes]\n",
        "\n",
        "# Print the datasets that were merged into 'eight_in_one.csv'\n",
        "print(\"Datasets merged into 'eight_in_one.csv':\")\n",
        "for dataset in merged_datasets:\n",
        "    print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYiwvfNFTTL1"
      },
      "outputs": [],
      "source": [
        "objects_df = pd.read_csv(\"/content/objects.csv\", dtype={'funding_total_usd_objects': float})\n",
        "eight_in_one_df = pd.read_csv(\"/content/eight_in_one.csv\", dtype={'funding_total_usd_objects': float})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7EM7gh8SSzJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the 'acquisitions,' 'investments,' and 'objects' datasets\n",
        "acquisitions_df = pd.read_csv(\"/content/acquisitions.csv\")\n",
        "investments_df = pd.read_csv(\"/content/investments.csv\")\n",
        "objects_df = pd.read_csv(\"/content/objects.csv\")\n",
        "\n",
        "# Load the 'eight_in_one' dataset\n",
        "eight_in_one_df = pd.read_csv(\"/content/eight_in_one.csv\")\n",
        "\n",
        "# Merge the datasets into one unified dataset using index\n",
        "unified_df = eight_in_one_df.merge(acquisitions_df, left_index=True, right_index=True, how='left', suffixes=('', '_acquisitions'))\n",
        "unified_df = unified_df.merge(investments_df, left_index=True, right_index=True, how='left', suffixes=('', '_investments'))\n",
        "unified_df = unified_df.merge(objects_df, left_index=True, right_index=True, how='left', suffixes=('', '_objects'))\n",
        "\n",
        "# Check the first few rows of the unified dataset\n",
        "print(unified_df.head())\n",
        "\n",
        "# Save the unified dataset to a CSV file\n",
        "unified_df.to_csv('/content/unified_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdlyxpLGUJot"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the unified dataset\n",
        "unified_df = pd.read_csv('/content/unified_dataset.csv')\n",
        "\n",
        "# Get the number of rows and columns\n",
        "num_rows, num_columns = unified_df.shape\n",
        "\n",
        "# Get the column names\n",
        "column_names = unified_df.columns.tolist()\n",
        "\n",
        "# Get basic statistics about the dataset\n",
        "dataset_info = unified_df.describe()\n",
        "\n",
        "# Print the information\n",
        "print(f\"Number of Rows: {num_rows}\")\n",
        "print(f\"Number of Columns: {num_columns}\")\n",
        "print(\"Column Names:\")\n",
        "for column in column_names:\n",
        "    print(f\"- {column}\")\n",
        "\n",
        "print(\"\\nBasic Statistics:\")\n",
        "print(dataset_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNJ6zqH4VErq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the unified dataset\n",
        "unified_df = pd.read_csv('/content/unified_dataset.csv')\n",
        "\n",
        "# Replace missing values in numeric columns with a specific value (e.g., 0)\n",
        "numeric_columns = unified_df.select_dtypes(include=['number']).columns\n",
        "unified_df[numeric_columns] = unified_df[numeric_columns].fillna(0)\n",
        "\n",
        "# Replace missing values in categorical columns with a specific value (e.g., 'Unknown')\n",
        "categorical_columns = unified_df.select_dtypes(exclude=['number']).columns\n",
        "unified_df[categorical_columns] = unified_df[categorical_columns].fillna('Unknown')\n",
        "\n",
        "\n",
        "#unified_df = unified_df.dropna()\n",
        "\n",
        "# Save the cleaned dataset to a new CSV file\n",
        "unified_df.to_csv('/content/cleaned_unified_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaUE-3U5nK4K"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost\n",
        "!pip install lightgbm\n",
        "!pip install catboost\n",
        "\n",
        "# Data Manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Additional ML libraries\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "\n",
        "# Set the style of the visualization\n",
        "sns.set(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKn3uQT6sD0_"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "file_path = '/content/cleaned_unified_dataset.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Get a concise summary of the dataframe\n",
        "print(\"\\nDataframe Information:\")\n",
        "print(data.info())\n",
        "\n",
        "# Basic statistical details\n",
        "print(\"\\nBasic Statistical Details:\")\n",
        "print(data.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH3rPgbotIM_"
      },
      "outputs": [],
      "source": [
        "# Finding rows with unique startup names\n",
        "unique_startups = data.drop_duplicates(subset=['name'])\n",
        "\n",
        "# Displaying the rows with unique startup names\n",
        "print(\"Rows with Unique Startup Names:\")\n",
        "print(unique_startups)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCmnupHFuX9K"
      },
      "outputs": [],
      "source": [
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the unique startup dataset:\")\n",
        "print(unique_startups.head())\n",
        "\n",
        "# Summarize the dataset\n",
        "print(\"\\nSummary of the unique startup dataset:\")\n",
        "print(unique_startups.info())\n",
        "\n",
        "# Display basic statistical details for numerical columns\n",
        "print(\"\\nBasic Statistical Details:\")\n",
        "print(unique_startups.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1LheCSYvbTU"
      },
      "outputs": [],
      "source": [
        "# Display the column names\n",
        "print(\"Column Names:\")\n",
        "print(unique_startups.columns)\n",
        "\n",
        "# Check for null values in each column\n",
        "print(\"\\nCount of Null Values in Each Column:\")\n",
        "print(unique_startups.isnull().sum())\n",
        "\n",
        "# Check for duplicate rows\n",
        "duplicate_rows = unique_startups.duplicated().sum()\n",
        "print(\"\\nNumber of Duplicate Rows:\")\n",
        "print(duplicate_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rD2WFtWv4w6"
      },
      "outputs": [],
      "source": [
        "# Counting the number of startups in the dataset\n",
        "number_of_startups = unique_startups.shape[0]\n",
        "\n",
        "print(\"Number of unique startups in the dataset:\", number_of_startups)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5EfRPdJwbws"
      },
      "outputs": [],
      "source": [
        "# Function to find duplicate columns\n",
        "def get_duplicate_columns(unique_startups):\n",
        "    duplicate_columns = []\n",
        "    for i in range(unique_startups.shape[1]):\n",
        "        col1 = unique_startups.iloc[:, i]\n",
        "        for j in range(i + 1, unique_startups.shape[1]):\n",
        "            col2 = unique_startups.iloc[:, j]\n",
        "            if col1.equals(col2):\n",
        "                duplicate_columns.append(unique_startups.columns.values[j])\n",
        "    return duplicate_columns\n",
        "\n",
        "# Find and print duplicate columns\n",
        "duplicates = get_duplicate_columns(unique_startups)\n",
        "print(\"Duplicate Columns:\", duplicates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGwCdKN1xWVK"
      },
      "outputs": [],
      "source": [
        "# List of duplicate columns - replace this list with the one you have found\n",
        "duplicate_columns = ['description_offices', 'address1', 'address2', 'zip_code', 'state_code_offices', 'fund_id', 'raised_currency_code_funds', 'updated_at_funds', 'degree_type', 'subject', 'institution', 'graduated_at', 'created_at_degrees', 'updated_at_degrees', 'id_funding_rounds', 'funding_round_id', 'funded_at_funding_rounds', 'funding_round_type', 'funding_round_code', 'raised_amount_usd', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'subject', 'institution', 'graduated_at', 'created_at_degrees', 'updated_at_degrees', 'id_funding_rounds', 'funding_round_id', 'funded_at_funding_rounds', 'funding_round_type', 'funding_round_code', 'raised_amount_usd', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'institution', 'graduated_at', 'created_at_degrees', 'updated_at_degrees', 'id_funding_rounds', 'funding_round_id', 'funded_at_funding_rounds', 'funding_round_type', 'funding_round_code', 'raised_amount_usd', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'graduated_at', 'created_at_degrees', 'updated_at_degrees', 'id_funding_rounds', 'funding_round_id', 'funded_at_funding_rounds', 'funding_round_type', 'funding_round_code', 'raised_amount_usd', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'created_at_degrees', 'updated_at_degrees', 'id_funding_rounds', 'funding_round_id', 'funded_at_funding_rounds', 'funding_round_type', 'funding_round_code', 'raised_amount_usd', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'updated_at_degrees', 'id_funding_rounds', 'funding_round_id', 'funded_at_funding_rounds', 'funding_round_type', 'funding_round_code', 'raised_amount_usd', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'id_funding_rounds', 'funding_round_id', 'funded_at_funding_rounds', 'funding_round_type', 'funding_round_code', 'raised_amount_usd', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'funding_round_id', 'funded_at_funding_rounds', 'funding_round_type', 'funding_round_code', 'raised_amount_usd', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'funded_at_funding_rounds', 'funding_round_type', 'funding_round_code', 'raised_amount_usd', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'funding_round_type', 'funding_round_code', 'raised_amount_usd', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'funding_round_code', 'raised_amount_usd', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'raised_amount_usd', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'raised_amount_funding_rounds', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'raised_currency_code_funding_rounds', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'post_money_valuation', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'participants', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'is_first_round', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'is_last_round', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'source_url_funding_rounds', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'source_description_funding_rounds', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'created_by_funding_rounds', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'created_at_funding_rounds', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'updated_at_funding_rounds', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'valuation_amount', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'fund_id_funds', 'updated_at_funds.1', 'raised_amount_ipos', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'public_at', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'source_url_ipos', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'source_description_ipos', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'latitude', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'address1', 'address2', 'zip_code', 'state_code_offices', 'city_offices', 'address2', 'zip_code', 'state_code_offices', 'zip_code', 'state_code_offices', 'state_code_offices', 'longitude', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'created_at_offices', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'updated_at_offices', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'id_people', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'first_name', 'last_name', 'birthplace', 'affiliation_name', 'last_name', 'birthplace', 'affiliation_name', 'birthplace', 'affiliation_name', 'affiliation_name']  # and so on...\n",
        "\n",
        "# Drop duplicate columns\n",
        "df = unique_startups.drop(columns=duplicate_columns)\n",
        "\n",
        "# Save the cleaned dataset\n",
        "df.to_csv('/content/unique_startups.csv', index=False)\n",
        "\n",
        "# Optional: Display basic information about the cleaned dataframe\n",
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset from Google Colab path\n",
        "df = pd.read_csv('/content/unique_startups.csv')\n",
        "\n",
        "# 3.3.1 Descriptive Statistics\n",
        "print(\"Descriptive Statistics:\\n\", df.describe())\n",
        "print(\"\\nFrequency of Categorical Variables:\\n\", df.select_dtypes(include=['object']).apply(pd.Series.value_counts))\n",
        "\n",
        "# 3.3.2 Visualization of Distributions\n",
        "# Continuous Variables\n",
        "for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    df[col].hist()\n",
        "    plt.title(f'Histogram of {col}')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    df.boxplot(column=[col])\n",
        "    plt.title(f'Box Plot of {col}')\n",
        "    plt.show()\n",
        "\n",
        "# Categorical Variables\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.countplot(y=col, data=df)\n",
        "    plt.title(f'Bar Chart of {col}')\n",
        "    plt.show()\n",
        "\n",
        "# 3.3.3 Correlation Analysis\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='viridis')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "#  Cross-Variable Analysis\n",
        "# Scatter plots for continuous variables\n",
        "continuous_vars = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "for i in range(len(continuous_vars)):\n",
        "    for j in range(i+1, len(continuous_vars)):\n",
        "        plt.scatter(df[continuous_vars[i]], df[continuous_vars[j]])\n",
        "        plt.title(f'Scatter Plot of {continuous_vars[i]} vs {continuous_vars[j]}')\n",
        "        plt.xlabel(continuous_vars[i])\n",
        "        plt.ylabel(continuous_vars[j])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Anomaly Detection\n",
        "\n",
        "\n",
        "target_column = 'funding_total_usd' if 'funding_total_usd' in df.columns else df.columns[0]\n",
        "Q1 = df[target_column].quantile(0.25)\n",
        "Q3 = df[target_column].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = df[(df[target_column] < (Q1 - 1.5 * IQR)) | (df[target_column] > (Q3 + 1.5 * IQR))]\n",
        "print(\"Outliers in the dataset:\\n\", outliers)\n"
      ],
      "metadata": {
        "id": "lHjC5IOhfPaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkCL_RKLyo6V"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Identifying numerical columns\n",
        "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "print(\"Numerical Columns:\")\n",
        "print(numerical_columns)\n",
        "\n",
        "# Identifying categorical columns\n",
        "# 'object' often indicates categorical data, but it can also include strings and mixed types\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "print(\"\\nCategorical Columns:\")\n",
        "print(categorical_columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nvzcGBr4pFN"
      },
      "outputs": [],
      "source": [
        "# Define the success criteria\n",
        "success_criteria = ['acquired', 'ipo']\n",
        "\n",
        "# Filter the DataFrame for successful startups\n",
        "successful_startups = df[df['status'].isin(success_criteria)]\n",
        "\n",
        "# Display the successful startups\n",
        "print(\"Successful Startups:\")\n",
        "print(successful_startups)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJy2r4qM7GIa"
      },
      "outputs": [],
      "source": [
        "# Assuming 'df' is your DataFrame and 'status' is the column indicating success\n",
        "# Define success criteria\n",
        "success_criteria = ['acquired', 'ipo']\n",
        "\n",
        "# Count successful startups\n",
        "successful_startup_count = df[df['status'].isin(success_criteria)].shape[0]\n",
        "\n",
        "# Count other startups\n",
        "other_startup_count = df[~df['status'].isin(success_criteria)].shape[0]\n",
        "\n",
        "print(f\"Number of successful startups: {successful_startup_count}\")\n",
        "print(f\"Number of other startups: {other_startup_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnGerAuG75XN"
      },
      "outputs": [],
      "source": [
        "# List of potential features\n",
        "potential_features = ['funding_total_usd', 'funding_rounds', 'founded_at', 'category_code', 'country_code', 'city', 'milestones', 'relationships']\n",
        "\n",
        "# Check if these features exist in your dataframe\n",
        "available_features = [feature for feature in potential_features if feature in df.columns]\n",
        "\n",
        "print(\"Available Features for the Model:\", available_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset from Google Colab path\n",
        "df = pd.read_csv('/content/unique_startups.csv')\n",
        "\n",
        "# 3.3.1 Descriptive Statistics\n",
        "print(\"Descriptive Statistics:\\n\", df.describe())\n",
        "print(\"\\nFrequency of Categorical Variables:\\n\", df.select_dtypes(include=['object']).apply(pd.Series.value_counts))\n",
        "\n",
        "# 3.3.2 Visualization of Distributions\n",
        "# Continuous Variables\n",
        "for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    df[col].hist()\n",
        "    plt.title(f'Histogram of {col}')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    df.boxplot(column=[col])\n",
        "    plt.title(f'Box Plot of {col}')\n",
        "    plt.show()\n",
        "\n",
        "# Categorical Variables\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.countplot(y=col, data=df)\n",
        "    plt.title(f'Bar Chart of {col}')\n",
        "    plt.show()\n",
        "\n",
        "# Correlation Analysis\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='viridis')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "#  Cross-Variable Analysis\n",
        "# Scatter plots for continuous variables\n",
        "continuous_vars = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "for i in range(len(continuous_vars)):\n",
        "    for j in range(i+1, len(continuous_vars)):\n",
        "        plt.scatter(df[continuous_vars[i]], df[continuous_vars[j]])\n",
        "        plt.title(f'Scatter Plot of {continuous_vars[i]} vs {continuous_vars[j]}')\n",
        "        plt.xlabel(continuous_vars[i])\n",
        "        plt.ylabel(continuous_vars[j])\n",
        "        plt.show()\n",
        "\n",
        "#  Temporal Analysis\n",
        "# This section is dependent on whether your dataset contains date/time columns.\n",
        "# Adjust the code as needed based on your dataset's structure.\n",
        "\n",
        "# Anomaly Detection\n",
        "\n",
        "target_column = 'funding_total_usd' if 'funding_total_usd' in df.columns else df.columns[0]  # Replace with your actual column name\n",
        "Q1 = df[target_column].quantile(0.25)\n",
        "Q3 = df[target_column].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = df[(df[target_column] < (Q1 - 1.5 * IQR)) | (df[target_column] > (Q3 + 1.5 * IQR))]\n",
        "print(\"Outliers in the dataset:\\n\", outliers)\n"
      ],
      "metadata": {
        "id": "hZOK3reqnKhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67yWglpl3Uyb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Load your dataset\n",
        "# Replace this with the path to your dataset\n",
        "data = pd.read_csv('/content/unique_startups.csv')\n",
        "\n",
        "# Select the columns for ANOVA\n",
        "anova_data = data[['category_code', 'funding_total_usd']]\n",
        "\n",
        "# Remove rows with missing values for these columns\n",
        "anova_data = anova_data.dropna()\n",
        "\n",
        "# Assuming 'category_code' is the categorical variable and we want to compare 'funding_total_usd' across categories\n",
        "groups = anova_data.groupby('category_code')['funding_total_usd'].apply(list)\n",
        "\n",
        "# Perform the ANOVA test\n",
        "# *groups.values unpacks the groups into separate arguments for the f_oneway function\n",
        "f_statistic, p_value = stats.f_oneway(*groups.values)\n",
        "\n",
        "print(f\"F-Statistic: {f_statistic}, P-Value: {p_value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2gLaCfH7Orv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/unique_startups.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# List of potential features for the model\n",
        "potential_features = ['funding_total_usd', 'funding_rounds', 'founded_at', 'category_code', 'country_code', 'city', 'milestones', 'relationships']\n",
        "\n",
        "# Define the success criteria\n",
        "success_criteria = ['acquired', 'ipo']\n",
        "\n",
        "# Filter the DataFrame for successful startups\n",
        "successful_startups = data[data['status'].isin(success_criteria)]\n",
        "\n",
        "# Identifying numerical and categorical columns among the selected features\n",
        "numerical_columns = successful_startups.select_dtypes(include=['int64', 'float64']).columns.intersection(potential_features)\n",
        "categorical_columns = successful_startups.select_dtypes(include=['object']).columns.intersection(potential_features)\n",
        "\n",
        "# Dictionary to store ANOVA results\n",
        "anova_results = {}\n",
        "\n",
        "# Perform ANOVA for each pair of categorical and numerical columns among selected features\n",
        "for num_col in numerical_columns:\n",
        "    for cat_col in categorical_columns:\n",
        "        # Grouping data\n",
        "        groups = successful_startups.groupby(cat_col)[num_col].apply(list)\n",
        "\n",
        "        # Check if there are at least two groups with more than one observation\n",
        "        valid_groups = [group for group in groups if len(group) > 1]\n",
        "        if len(valid_groups) > 1:\n",
        "            f_statistic, p_value = stats.f_oneway(*valid_groups)\n",
        "            anova_results[(cat_col, num_col)] = (f_statistic, p_value)\n",
        "\n",
        "# Print the ANOVA results\n",
        "for pair, result in anova_results.items():\n",
        "    cat_col, num_col = pair\n",
        "    f_statistic, p_value = result\n",
        "    print(f\"ANOVA for {cat_col} and {num_col}: F-Statistic = {f_statistic}, P-Value = {p_value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEMKLHzk8QJE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data - replace this with your actual DataFrame\n",
        "# Assuming you have a DataFrame 'df' with these columns\n",
        "df = pd.DataFrame({\n",
        "    'funding_rounds': np.random.randint(1, 10, 100),\n",
        "    'relationships': np.random.randint(1, 20, 100),\n",
        "    'funding_total_usd': np.random.uniform(1e6, 1e9, 100),\n",
        "    'success_probability': np.random.uniform(0, 1, 100)\n",
        "})\n",
        "\n",
        "# Calculating average success probability for each 'funding_rounds'\n",
        "avg_success_by_funding_rounds = df.groupby('funding_rounds')['success_probability'].mean().reset_index()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Bar plot for average success probability\n",
        "sns.barplot(x='funding_rounds', y='success_probability', data=avg_success_by_funding_rounds, palette='viridis')\n",
        "\n",
        "# Density plot overlaid for distribution\n",
        "sns.kdeplot(df['funding_rounds'], bw_adjust=0.5, color='red', fill=True, alpha=0.3)\n",
        "\n",
        "plt.title('Average Success Probability by Funding Rounds with Distribution Overlay')\n",
        "plt.xlabel('Funding Rounds')\n",
        "plt.ylabel('Average Success Probability')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgncUjTM8svh"
      },
      "outputs": [],
      "source": [
        "# Handle missing values for numeric columns only\n",
        "df.fillna(df.mean(numeric_only=True), inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFZnHCI03Bbw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Features identified for the model\n",
        "selected_features = ['funding_total_usd', 'funding_rounds', 'founded_at', 'category_code', 'country_code', 'city', 'milestones', 'relationships']\n",
        "\n",
        "# Assuming 'status' is the column indicating a successful startup\n",
        "data['is_successful'] = data['status'].apply(lambda x: 1 if x in ['acquired', 'ipo'] else 0)\n",
        "\n",
        "# Encoding categorical variables and handling missing values\n",
        "label_encoders = {}\n",
        "for column in data[selected_features].select_dtypes(include=['object']).columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    data[column] = label_encoders[column].fit_transform(data[column].astype(str))\n",
        "\n",
        "# Handle missing values - here we fill them with the mean. Consider a more appropriate strategy for your dataset.\n",
        "#data.fillna(data.mean(), inplace=True)\n",
        "\n",
        "# Splitting the dataset\n",
        "X = data[selected_features]\n",
        "y = data['is_successful']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Training the Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluating the model\n",
        "predictions = model.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "# Identifying top 5 most important features\n",
        "feature_importances = pd.DataFrame(model.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(feature_importances.head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_btCe5FL9CaP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming feature_importances is your DataFrame containing the feature importances\n",
        "feature_importances = pd.DataFrame({'feature': ['city', 'founded_at', 'relationships', 'category_code', 'funding_total_usd'],\n",
        "                                    'importance': [0.240962, 0.191549, 0.159871, 0.153828, 0.077119]})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importances)\n",
        "plt.title('Top 5 Important Features for Predicting Startup Success')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQIHOSJTy7O9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace this with your actual file path\n",
        "file_path = '/content/unique_startups.csv'\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"File loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please check the file path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgRWtABh9m2g"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import RFE\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "selected_features = ['funding_total_usd', 'funding_rounds', 'founded_at', 'category_code', 'country_code', 'city', 'milestones', 'relationships']\n",
        "\n",
        "# Encoding categorical variables and handling missing values\n",
        "label_encoders = {}\n",
        "for column in df[selected_features].select_dtypes(include=['object']).columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    df[column] = label_encoders[column].fit_transform(df[column].astype(str))\n",
        "\n",
        "# Handle missing values\n",
        "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
        "\n",
        "# Define the target variable\n",
        "df['is_successful'] = df['status'].apply(lambda x: 1 if x in ['acquired', 'ipo'] else 0)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "df[selected_features] = scaler.fit_transform(df[selected_features])\n",
        "\n",
        "# Splitting the dataset\n",
        "X = df[selected_features]\n",
        "y = df['is_successful']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Training the SVM Classifier\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluating the model\n",
        "predictions = svm_model.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "# Applying RFE for feature selection\n",
        "selector = RFE(svm_model, n_features_to_select=5, step=1)\n",
        "selector = selector.fit(X_train, y_train)\n",
        "\n",
        "# Identifying top features\n",
        "top_features = pd.Series(selector.support_, index=X.columns)\n",
        "print(\"Top Features according to RFE:\")\n",
        "print(top_features[top_features == True].index.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p9sPdhz-C-m"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'top_features' is a boolean series from RFE indicating selected features\n",
        "# Replace this with your actual RFE results\n",
        "top_features = pd.Series([True, False, True, True, False, True, True, False],\n",
        "                         index=['funding_total_usd', 'funding_rounds', 'founded_at', 'category_code', 'country_code', 'city', 'milestones', 'relationships'])\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "features_df = pd.DataFrame({'Feature': top_features.index, 'Selected': top_features.values})\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Selected', y='Feature', data=features_df)\n",
        "plt.title('Top Features Selected by RFE with SVM Classifier')\n",
        "plt.xlabel('Selected')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flK8WoEh-YRk"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Coefficients as feature importance\n",
        "log_reg_importance = pd.DataFrame({'feature': X_train.columns, 'importance': abs(log_reg.coef_[0])})\n",
        "log_reg_importance.sort_values(by='importance', ascending=False, inplace=True)\n",
        "print(\"Logistic Regression Feature Importance:\\n\", log_reg_importance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oyS4y5s-g8G"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Train the Gradient Boosting model\n",
        "gb_clf = GradientBoostingClassifier()\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance\n",
        "gb_importance = pd.DataFrame({'feature': X_train.columns, 'importance': gb_clf.feature_importances_})\n",
        "gb_importance.sort_values(by='importance', ascending=False, inplace=True)\n",
        "print(\"Gradient Boosting Feature Importance:\\n\", gb_importance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UliEdzgF-mzv"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance\n",
        "xgb_importance = pd.DataFrame({'feature': X_train.columns, 'importance': xgb_clf.feature_importances_})\n",
        "xgb_importance.sort_values(by='importance', ascending=False, inplace=True)\n",
        "print(\"XGBoost Feature Importance:\\n\", xgb_importance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mdTeIBv-_dl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have dataframes 'log_reg_importance', 'gb_importance', and 'xgb_importance' from the respective models\n",
        "\n",
        "# Function to create a bar plot for feature importance\n",
        "def plot_feature_importance(importance_df, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='importance', y='feature', data=importance_df)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.show()\n",
        "\n",
        "# Plot for Logistic Regression\n",
        "plot_feature_importance(log_reg_importance, 'Logistic Regression Feature Importance')\n",
        "\n",
        "# Plot for Gradient Boosting\n",
        "plot_feature_importance(gb_importance, 'Gradient Boosting Feature Importance')\n",
        "\n",
        "# Plot for XGBoost\n",
        "plot_feature_importance(xgb_importance, 'XGBoost Feature Importance')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANXqkIIZ_QuB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create dataframes for each model's feature importance (assuming these are your results)\n",
        "rf_importance = pd.DataFrame({'feature': ['city', 'founded_at', 'relationships', 'category_code', 'funding_total_usd'], 'importance': [0.240962, 0.191549, 0.159871, 0.153828, 0.077119], 'model': 'Random Forest'})\n",
        "log_reg_importance = pd.DataFrame({'feature': ['relationships', 'milestones', 'country_code', 'founded_at', 'category_code'], 'importance': [0.459277, 0.205762, 0.203600, 0.091770, 0.090510], 'model': 'Logistic Regression'})\n",
        "gb_importance = pd.DataFrame({'feature': ['founded_at', 'relationships', 'milestones', 'category_code', 'funding_total_usd'], 'importance': [0.292110, 0.247738, 0.162949, 0.109068, 0.089996], 'model': 'Gradient Boosting'})\n",
        "xgb_importance = pd.DataFrame({'feature': ['milestones', 'founded_at', 'relationships', 'category_code', 'country_code'], 'importance': [0.178565, 0.156499, 0.154944, 0.127769, 0.115070], 'model': 'XGBoost'})\n",
        "svm_rfe_importance = pd.DataFrame({'feature': ['funding_total_usd', 'funding_rounds', 'category_code', 'milestones', 'relationships'], 'importance': [1, 1, 1, 1, 1], 'model': 'SVM with RFE'})  # Equal importance to selected features\n",
        "\n",
        "# Normalize the importance scores\n",
        "scaler = MinMaxScaler()\n",
        "combined = pd.concat([rf_importance, log_reg_importance, gb_importance, xgb_importance, svm_rfe_importance])\n",
        "combined['importance_normalized'] = scaler.fit_transform(combined[['importance']])\n",
        "\n",
        "# Aggregate the scores and identify top 3 features\n",
        "aggregate_importance = combined.groupby('feature')['importance_normalized'].mean().reset_index()\n",
        "top_features = aggregate_importance.sort_values('importance_normalized', ascending=False).head(3)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance_normalized', y='feature', data=top_features)\n",
        "plt.title('Top 3 Important Features Across Models')\n",
        "plt.xlabel('Normalized Aggregate Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilaxXBNNAlJG"
      },
      "outputs": [],
      "source": [
        "\n",
        "top_features = ['funding_rounds', 'relationships', 'funding_total_usd']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZl9K7qpnnq2"
      },
      "outputs": [],
      "source": [
        "X_train_top = X_train[top_features]\n",
        "X_test_top = X_test[top_features]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvNkRxVnnrMH"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize models\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "svm_clf = SVC(kernel='linear', random_state=42)\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "gb_clf = GradientBoostingClassifier(random_state=42)\n",
        "xgb_clf = xgb.XGBClassifier(random_state=42)\n",
        "\n",
        "# Dictionary to store predictions\n",
        "predictions = {}\n",
        "\n",
        "# Train and predict with each model\n",
        "for clf, name in zip([rf_clf, svm_clf, log_reg, gb_clf, xgb_clf],\n",
        "                     ['Random Forest', 'SVM', 'Logistic Regression', 'Gradient Boosting', 'XGBoost']):\n",
        "    clf.fit(X_train_top, y_train)\n",
        "    preds = clf.predict(X_test_top)\n",
        "    predictions[name] = preds\n",
        "    accuracy = accuracy_score(y_test, preds)\n",
        "    print(f\"{name} Accuracy with top features: {accuracy:.4f}\")\n",
        "\n",
        "# Ensemble predictions (majority voting)\n",
        "ensemble_preds = pd.DataFrame(predictions).mode(axis=1)[0]\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_preds)\n",
        "print(f\"Ensemble Accuracy with top features: {ensemble_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNTPX_uAoMsN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Accuracy scores from your output\n",
        "accuracies = {\n",
        "    'Random Forest': 0.9611,\n",
        "    'SVM': 0.9648,\n",
        "    'Logistic Regression': 0.9647,\n",
        "    'Gradient Boosting': 0.9647,\n",
        "    'XGBoost': 0.9637,\n",
        "    'Ensemble': 0.9647\n",
        "}\n",
        "\n",
        "# Convert to DataFrame for plotting\n",
        "accuracy_df = pd.DataFrame(list(accuracies.items()), columns=['Model', 'Accuracy'])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Accuracy', y='Model', data=accuracy_df.sort_values('Accuracy', ascending=False), palette='viridis')\n",
        "plt.title('Model Accuracies with Top Features')\n",
        "plt.xlabel('Accuracy Score')\n",
        "plt.ylabel('Model')\n",
        "plt.xlim(0.92, 0.98)  # Adjust the x-axis limits to zoom in on differences\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkRcHNJ3rrRO"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "\n",
        "# Define a set of hyperparameters for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'min_samples_split': [2, 4],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'subsample': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize the Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the GridSearchCV object to the data to compute the best model\n",
        "grid_search.fit(X_train_top, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters:\\n\", grid_search.best_params_)\n",
        "\n",
        "# Use the best estimator for making predictions\n",
        "best_gb_clf = grid_search.best_estimator_\n",
        "\n",
        "# Predictions with the tuned model\n",
        "tuned_predictions = best_gb_clf.predict(X_test_top)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "print(\"Tuned Model Accuracy:\", accuracy_score(y_test, tuned_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trAn9L6rouRo"
      },
      "outputs": [],
      "source": [
        "\n",
        "df['is_successful'] = df['status'].apply(lambda x: 1 if x in ['acquired', 'ipo'] else 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzkbD0mnhygV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Top features identified previously\n",
        "top_features = ['funding_rounds', 'relationships', 'funding_total_usd']\n",
        "\n",
        "# Splitting the dataset using only the top features\n",
        "X = df[top_features]\n",
        "y = df['is_successful']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define a set of hyperparameters for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'min_samples_split': [2, 4],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'subsample': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize the Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the GridSearchCV object to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters:\\n\", grid_search.best_params_)\n",
        "\n",
        "# Use the best estimator for making predictions\n",
        "best_gb_clf = grid_search.best_estimator_\n",
        "\n",
        "# Predictions with the best model\n",
        "y_pred = best_gb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DXs6soIwtHL"
      },
      "outputs": [],
      "source": [
        "# Assuming 'top_features' is the target variable\n",
        "y_train = le.fit_transform(y_train)  # Fit and transform on training data\n",
        "y_test = le.transform(y_test)        # Only transform on testing data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKqCGyfLxCYD"
      },
      "outputs": [],
      "source": [
        "# Print out the data types of the columns in your dataframe\n",
        "print(X_train.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkXIsgADxjON"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Apply label encoding to each object-type column except the target column\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object' and column != 'is_successful':\n",
        "        # Fill missing values with a placeholder\n",
        "        df[column] = df[column].fillna('Missing')\n",
        "        df[column] = le.fit_transform(df[column])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNDhbXYMxleb"
      },
      "outputs": [],
      "source": [
        "# Fill missing values in numeric columns with the median\n",
        "for column in df.columns:\n",
        "    if df[column].dtype in ['int64', 'float64']:\n",
        "        df[column] = df[column].fillna(df[column].median())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtpDACk8xqHs"
      },
      "outputs": [],
      "source": [
        "X = df.drop('is_successful', axis=1)  # Feature matrix\n",
        "y = df['is_successful']               # Target vector\n",
        "\n",
        "# If 'is_successful' is categorical, apply label encoding\n",
        "if y.dtype == 'object':\n",
        "    y = le.fit_transform(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj02ziQ2xs2v"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY0_3460yGGn"
      },
      "outputs": [],
      "source": [
        "# Print out the data types of the columns in X_train\n",
        "print(X_train.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FSstOMYyUwG"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "for column in df.select_dtypes(include=['object']).columns:\n",
        "    if column not in ['created_at_objects', 'updated_at_objects']:  # Exclude date columns for now\n",
        "        df[column] = df[column].astype(str)  # Convert to string\n",
        "        df[column] = le.fit_transform(df[column])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee7reXkLyWg7"
      },
      "outputs": [],
      "source": [
        "df['created_at_objects'] = pd.to_datetime(df['created_at_objects'], errors='coerce')\n",
        "df['updated_at_objects'] = pd.to_datetime(df['updated_at_objects'], errors='coerce')\n",
        "\n",
        "df['created_at_year'] = df['created_at_objects'].dt.year\n",
        "df['created_at_month'] = df['created_at_objects'].dt.month\n",
        "df['created_at_day'] = df['created_at_objects'].dt.day\n",
        "\n",
        "df['updated_at_year'] = df['updated_at_objects'].dt.year\n",
        "df['updated_at_month'] = df['updated_at_objects'].dt.month\n",
        "df['updated_at_day'] = df['updated_at_objects'].dt.day\n",
        "\n",
        "# Drop the original date columns\n",
        "df.drop(['created_at_objects', 'updated_at_objects'], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe1xeoPayeYW"
      },
      "outputs": [],
      "source": [
        "# Fill missing values\n",
        "df.fillna(df.median(), inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJAKl5OiyhG9"
      },
      "outputs": [],
      "source": [
        "X = df.drop('is_successful', axis=1)  # Replace 'is_successful' with your target column\n",
        "y = df['is_successful']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_Oiyz97jFg0"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/unique_startups.csv')\n",
        "\n",
        "df['is_successful'] = df['status'].apply(lambda x: 1 if x in ['acquired', 'ipo'] else 0)\n",
        "\n",
        "# Assume 'target' is the column name of your target variable\n",
        "X = df.drop('is_successful', axis=1)  # Feature matrix\n",
        "y = df['is_successful']               # Target variable\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Model refinement with GridSearchCV for SVM\n",
        "parameters_svm = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "svc = SVC()\n",
        "clf_svm = GridSearchCV(svc, parameters_svm)\n",
        "clf_svm.fit(X_train, y_train)\n",
        "\n",
        "# Model refinement for Logistic Regression\n",
        "parameters_lr = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
        "logreg = LogisticRegression()\n",
        "clf_lr = GridSearchCV(logreg, parameters_lr)\n",
        "clf_lr.fit(X_train, y_train)\n",
        "\n",
        "# Ensemble model (e.g., Random Forest)\n",
        "parameters_rf = {'n_estimators': [100, 200], 'max_depth': [10, 20]}\n",
        "rf = RandomForestClassifier()\n",
        "clf_rf = GridSearchCV(rf, parameters_rf)\n",
        "clf_rf.fit(X_train, y_train)\n",
        "\n",
        "# Model validation for each model\n",
        "for model, name in zip([clf_svm, clf_lr, clf_rf], ['SVM', 'Logistic Regression', 'Random Forest']):\n",
        "    predictions = model.predict(X_test)\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SHLZdJA0kIB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Suppress SettingWithCopyWarning\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xljuFSESv_Ih"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load new data from a CSV file\n",
        "# Replace 'path_to_new_data.csv' with the actual file path\n",
        "new_data = pd.read_csv('/content/startup data.csv')\n",
        "\n",
        "# Now, continue with the steps to prepare the data for prediction\n",
        "top_features = ['funding_rounds', 'relationships', 'funding_total_usd']\n",
        "new_data_top_features = new_data[top_features]\n",
        "new_data_top_features.fillna(new_data_top_features.mean(), inplace=True)\n",
        "\n",
        "# If you used any encoders (like LabelEncoder) during training, apply them to the relevant columns\n",
        "# e.g., new_data_top_features['category'] = label_encoder.transform(new_data_top_features['category'])\n",
        "\n",
        "# Predicting with the trained model (best_gb_clf in this case)\n",
        "predictions = best_gb_clf.predict(new_data_top_features)\n",
        "# Print the predictions\n",
        "print(predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfOMGNHS1Bwj"
      },
      "outputs": [],
      "source": [
        "# Get the probabilities of being a successful startup\n",
        "success_probabilities = best_gb_clf.predict_proba(new_data_top_features)[:, 1]\n",
        "\n",
        "# Add probabilities to the new_data DataFrame for reference\n",
        "new_data['success_probability'] = success_probabilities\n",
        "\n",
        "# Sort the DataFrame by success_probability in descending order\n",
        "top_startups = new_data.sort_values(by='success_probability', ascending=False)\n",
        "\n",
        "# Select the top ten startups\n",
        "top_ten_startups = top_startups.head(10)\n",
        "\n",
        "# Display the top ten startups\n",
        "print(top_ten_startups)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7vlt34v1rav"
      },
      "outputs": [],
      "source": [
        "# Assuming your DataFrame is named 'top_ten_startups'\n",
        "# Extracting the names of the top ten startups\n",
        "top_ten_startup_names = top_ten_startups['name'].tolist()\n",
        "\n",
        "# Print the names of the top ten startups\n",
        "for i, name in enumerate(top_ten_startup_names, start=1):\n",
        "    print(f\"{i}. {name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR_21bW62f4q"
      },
      "outputs": [],
      "source": [
        "# Assuming 'top_ten_startups' is your DataFrame\n",
        "# Extracting success probabilities for the top ten startups\n",
        "top_ten_success_probabilities = top_ten_startups['success_probability'].tolist()\n",
        "\n",
        "# Print the success probabilities for the top ten startups\n",
        "for i, prob in enumerate(top_ten_success_probabilities, start=1):\n",
        "    print(f\"{i}. {prob}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOG2jwaY2uWB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Names of the top ten startups\n",
        "startup_names = ['Dilithium Networks', 'Photobucket', 'Virident Systems',\n",
        "                 'Peer39', 'Cozi Group', 'Jingle Networks',\n",
        "                 'Jumptap', 'Blip', 'ExactTarget', 'MOG']\n",
        "\n",
        "# Success probabilities for each startup\n",
        "success_probabilities = [0.8609552376838987] * 10  # Replicating the same probability for all startups\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "df = pd.DataFrame({'Startup Name': startup_names, 'Success Probability': success_probabilities})\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Success Probability', y='Startup Name', data=df, palette='viridis')\n",
        "plt.title('Top 10 Startups Likely to Succeed (Equal Probabilities)')\n",
        "plt.xlabel('Success Probability')\n",
        "plt.ylabel('Startup Name')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMw0du0AyAAvkWCF5nbIDi3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}